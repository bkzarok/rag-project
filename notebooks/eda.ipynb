{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8129dff6-0c5c-439d-9407-83bb81c6839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BUCKET = \n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09acc3c7-bcce-4bf6-937a-773519b085a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "OUT = Path(\"../data/raw\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "urls = [\n",
    "    \"https://faolex.fao.org/docs/pdf/ssd127441.pdf\",\n",
    "    \"https://www.eia.gov/international/content/analysis/countries_long/Sudan_and_South_Sudan/pdf/Sudans%20CAB%20FY2024.pdf\",\n",
    "    \"https://verite.org/wp-content/uploads/2021/07/Verite-Country-Report-South-Sudan.pdf\",\n",
    "    \"https://www.usip.org/sites/default/files/2021-04/sr_493-conflict_and_crisis_in_south_sudans_equatoria.pdf\",\n",
    "    \"https://libraries.indiana.edu/libraries/pdf.js/web/viewer.html?file=https%3A%2F%2Flibraries.indiana.edu%2Fsites%2Fdefault%2Ffiles%2FSOUTH%2520SUDAN.pdf\",\n",
    "    \"https://www.u4.no/publications/south-sudan-overview-of-corruption-and-anti-corruption.pdf\",\n",
    "    \"https://documents1.worldbank.org/curated/en/099031825031520159/pdf/P500556-9f568594-b30c-4fcc-bb7a-b96df92ed5fd.pdf\",\n",
    "    \"https://www.state.gov/wp-content/uploads/2021/03/SOUTH-SUDAN-2020-HUMAN-RIGHTS-REPORT.pdf\",\n",
    "    \"https://assets.publishing.service.gov.uk/media/671267b5b40d67191077b398/South_Sudan_Toponymic_Factfile.pdf\",\n",
    "    \"https://bti-project.org/fileadmin/api/content/en/downloads/reports/country_report_2022_SSD.pdf\",\n",
    "    \"https://era.ed.ac.uk/bitstream/handle/1842/42410/Perceiving-Peace-in-a-Fragment-State-The-Case-of-South-Sudan-DIGITAL.pdf?sequence=1&isAllowed=y\",\n",
    "    \"https://www.brookings.edu/wp-content/uploads/2016/06/06-south-sudan.pdf\",\n",
    "    \"https://www.cfr.org/sites/default/files/pdf/2016/11/CSR77_Knopf_South%20Sudan.pdf\",\n",
    "    \"https://www.ssoar.info/ssoar/bitstream/handle/document/67602/ssoar-jlibertyintaff-2020-1-afriyie_et_al-Comprehensive_analysis_of_South_Sudan.pdf\",\n",
    "    \"https://journalistsresource.org/wp-content/uploads/2011/08/South-Sudan.pdf\",\n",
    "    \"https://ciaotest.cc.columbia.edu/journals/ambrev/ambrev1257/f_0029920_24220.pdf\",\n",
    "    \"https://sgp.fas.org/crs/row/R43344.pdf\",\n",
    "    \"https://docs.pca-cpa.org/2016/02/South-Sudan-Peace-Agreement-September-2018.pdf\",\n",
    "    \"https://ucdpged.uu.se/peaceagreements/fulltext/SD_120927_Cooperation%20Agreement%20between%20Sudan%20and%20South%20Sudan.pdf\",\n",
    "    \"https://www.rahs-open-lid.com/wp-content/uploads/2024/02/South-Sudan_-The-Untold-Story-from-Independence-to-Civil-War-PDFDrive-.pdf\",\n",
    "    \"https://carnegie-production-assets.s3.amazonaws.com/static/files/files__sudan_conflict.pdf\",\n",
    "    \"https://www.impact-se.org/wp-content/uploads/South-Sudan.pdf\",\n",
    "    \"https://eprints.lse.ac.uk/108888/1/McCrone_the_wars_in_South_Sudan_published.pdf\",\n",
    "    \"https://med.virginia.edu/family-medicine/wp-content/uploads/sites/285/2018/12/Azobou_South-Sudan-Refugee-Crisis-112018.pdf\",\n",
    "    \"https://riftvalley.net/wp-content/uploads/2018/06/RVI-The-Sudan-Handbook.pdf\",\n",
    "    \"https://www.congress.gov/crs_external_products/IF/PDF/IF10218/IF10218.14.pdf\",\n",
    "    \"https://docs.southsudanngoforum.org/sites/default/files/2018-01/Labour%20Act%202017.pdf\",\n",
    "    \"https://mojca.gov.ss/wp-content/uploads/2023/03/Registration-of-Business-Names-Act-7-of-2008.pdf\",\n",
    "    \"https://archive.doingbusiness.org/content/dam/doingBusiness/country/s/south-sudan/SSD.pdf\",\n",
    "    \"https://www.wto.org/english/thewto_e/acc_e/ssd_e/wtaccssd6_leg_1.pdf\",\n",
    "    \"https://mojca.gov.ss/wp-content/uploads/2023/03/Interim-Constitution-of-Southern-Sudan-2005.pdf\",\n",
    "    \"https://faolex.fao.org/docs/pdf/ssd127441.pdf\",\n",
    "    \"https://www.refworld.org/sites/default/files/attachments/531469ee6.pdf\",\n",
    "    \"https://www.homeaffairs.gov.au/foi/files/2021/fa-210700890-document-released.PDF\",\n",
    "    \"https://togetherwomenrise.org/wp-content/uploads/2018/08/SouthSudan.pdf\",\n",
    "    \"https://faolex.fao.org/docs/pdf/ssd199925.pdf\",\n",
    "    \"https://mofaic.gov.ss/wp-content/uploads/2023/03/Investment-Promotion-Act-2009.pdf\",\n",
    "]\n",
    "for i,u in enumerate(urls,1):\n",
    "    p = OUT / f\"doc_{i:03}.pdf\"\n",
    "    with requests.get(u, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        p.write_bytes(r.content)\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d815e5b-52d4-4fe1-8ab6-d53cb3d5f4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping doc_005.pdf: not a valid PDF (No /Root object! - Is this really a PDF?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdfpage:The PDF <_io.BufferedReader name='../data/raw/doc_025.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping doc_008.pdf: not a valid PDF (No /Root object! - Is this really a PDF?)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pathlib\n",
    "from pdfminer.high_level import extract_text as pdf_text\n",
    "from readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "INP = pathlib.Path(\"../data/raw\")\n",
    "OUT = pathlib.Path(\"../data/interim\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for p in INP.iterdir():\n",
    "    if p.suffix.lower() == \".pdf\":\n",
    "        try:\n",
    "            txt = pdf_text(open(p, 'rb'))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {p.name}: not a valid PDF ({e})\")\n",
    "            continue\n",
    "    else:\n",
    "        html = p.read_text(encoding='utf-8', errors='ignore')\n",
    "        main = Document(html).summary()\n",
    "        txt = BeautifulSoup(main, 'html.parser').get_text(\"\")\n",
    "    (OUT / f\"{p.stem}.txt\").write_text(txt, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13ad9a96-ff86-4863-876f-9eb17164b6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#chunk\n",
    "import json, uuid, re\n",
    "from pathlib import Path\n",
    "\n",
    "INP = Path(\"../data/interim\")\n",
    "OUT = Path(\"../data/processed\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "CHUNK, OVERLAP = 800, 120\n",
    "\n",
    "for p in INP.glob(\"*.txt\"):\n",
    "    try:\n",
    "        t = re.sub(r\"\\n{3,}\", \"\\n\\n\", p.read_text(encoding='utf-8')).strip()\n",
    "        i, parts = 0, []\n",
    "        while i < len(t):\n",
    "            parts.append(t[i:i+CHUNK])\n",
    "            i += CHUNK - OVERLAP\n",
    "        with open(OUT / f\"{p.stem}.jsonl\", 'w', encoding='utf-8') as f:\n",
    "            for k, c in enumerate(parts):\n",
    "                try:\n",
    "                    rec = {\n",
    "                        \"chunk_id\": str(uuid.uuid4()),\n",
    "                        \"source\": p.stem,\n",
    "                        \"order\": k,\n",
    "                        \"text\": c\n",
    "                    }\n",
    "                    f.write(json.dumps(rec) + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error writing chunk {k} of {p.name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {p.name}: {e}\")                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bb81684-b693-4036-b4b4-66425a8672e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#enrich       \n",
    "import spacy, json\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for jf in Path(\"../data/processed\").glob(\"*.jsonl\"):\n",
    "    try:\n",
    "        lines = [json.loads(x) for x in open(jf, encoding='utf-8')]\n",
    "        for r in lines:\n",
    "            try:\n",
    "                doc = nlp(r[\"text\"])\n",
    "                r[\"entities\"] = [(e.text, e.label_) for e in doc.ents]\n",
    "            except Exception as e:\n",
    "                print(f\"NER error in {jf.name} chunk {r.get('order', '?')}: {e}\")\n",
    "                r[\"entities\"] = []\n",
    "        with open(jf, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(json.dumps(r) for r in lines))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {jf.name}: {e}\")                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7745298-6721-41b7-84a4-aaa3b3531e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0135618e7b3c43dcab7b6390ad2ca690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97d2ad146254a9d9d26843e6a327868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9525c68b86426c8276dd1397fcdcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9b951f903e410b9c9beac3a832bb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36501dff0c524d3db58358530ecf0c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6945dab03f344519a5283c06d7d1b461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c3f33615ea40b8a5352ceec20d2257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1bd0651e9f47af87c3219220000c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37702e7a18da4420a8fed86a877845d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7380e69d6de945748b5429919aed9552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954ef4e8531140bc96043e2779a95d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embed & FAISS index\n",
    "import faiss, numpy as np, json\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "chunks, metas = [], []\n",
    "\n",
    "for jf in Path(\"../data/processed\").glob(\"*.jsonl\"):\n",
    "    try:\n",
    "        with open(jf, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    chunks.append(rec['text'])\n",
    "                    metas.append({k: rec[k] for k in (\"chunk_id\", \"source\", \"order\")})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing line in {jf.name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {jf.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X = model.encode(chunks, batch_size=64, normalize_embeddings=True).astype('float32')\n",
    "except Exception as e:\n",
    "    print(f\"Error during embedding: {e}\")\n",
    "    X = np.zeros((len(chunks), 384), dtype='float32')  # fallback shape\n",
    "\n",
    "try:\n",
    "    index = faiss.IndexFlatIP(X.shape[1])\n",
    "    index.add(X)\n",
    "    Path(\"../data/index\").mkdir(parents=True, exist_ok=True)\n",
    "    faiss.write_index(index, \"../data/index/faiss.index\")\n",
    "    json.dump({\"chunks\": chunks}, open(\"../data/index/chunks.json\", \"w\", encoding='utf-8'))\n",
    "    json.dump({\"metas\": metas}, open(\"../data/index/meta.json\", \"w\", encoding='utf-8'))\n",
    "except Exception as e:\n",
    "    print(f\"Error during FAISS indexing or saving: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b462b6-4b86-4479-9a97-153a67b05c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search helper\n",
    "import faiss, numpy as np, json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading embedding model: {e}\")\n",
    "\n",
    "try:\n",
    "    index = faiss.read_index(\"../data/index/faiss.index\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading FAISS index: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(\"../data/index/chunks.json\", encoding=\"utf-8\") as f:\n",
    "        chunks = json.load(f)['chunks']\n",
    "    with open(\"../data/index/meta.json\", encoding=\"utf-8\") as f:\n",
    "        metas = json.load(f)['metas']\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading chunk/meta files: {e}\")\n",
    "\n",
    "def topk(query, k=5):\n",
    "    try:\n",
    "        qv = model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "        D, I = index.search(qv, k)\n",
    "        return [{\"score\": float(D[0][j]), \"text\": chunks[i], \"meta\": metas[i]} for j, i in enumerate(I[0])]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79069e7d-6108-479e-bf3c-b4c96c749db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt builder\n",
    "SYSTEM = (\n",
    "    \"Answer using ONLY the provided context. Cite sources inline like [1], [2]. \"\n",
    "    \"If not in context, say you cannot find it.\"\n",
    ")\n",
    "\n",
    "def build_prompt(q, hits):\n",
    "        ctx = []\n",
    "        for i, h in enumerate(hits, 1):\n",
    "                try:\n",
    "                        src = f\"{h['meta']['source']}#chunk{h['meta']['order']}\"\n",
    "                        text = h.get('text', '')\n",
    "                        ctx.append(f\"[{i}] {src}:\\n{text}\")\n",
    "                except Exception as e:\n",
    "                        print(f\"Error building context for hit {i}: {e}\")\n",
    "                        continue\n",
    "        try:\n",
    "                prompt = SYSTEM + \"\\n\\n\" + \"\\n\\n\".join(ctx) + f\"\\n\\nQuestion: {q}\\nAnswer with citations.\"\n",
    "                return prompt\n",
    "        except Exception as e:\n",
    "                print(f\"Error building prompt: {e}\")\n",
    "                return SYSTEM + f\"\\n\\nQuestion: {q}\\nAnswer with citations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76236d88-4ca1-432d-a02a-e609a07dfa6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or 'exit' to quit):  When did south sudan gain independence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " South Sudan gained independence on July 9, 2011, following a referendum held in January 2011, where 98.83% of the voters supported independence [2], [5].\n",
      "\n",
      "Sources:\n",
      "[1] doc_012#chunk5\n",
      "[2] doc_010#chunk8\n",
      "[3] doc_025#chunk25\n",
      "[4] doc_016#chunk4\n",
      "[5] doc_024#chunk3\n",
      "----------------------------------------\n",
      "\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "#LLM call \n",
    "\n",
    "from openai import OpenAI, APIError\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def answer(q, k=5):\n",
    "    try:\n",
    "        hits = topk(q, k)\n",
    "        prompt = build_prompt(q, hits)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", temperature=0.2,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a careful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return resp.choices[0].message.content, hits\n",
    "    except APIError as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return \"Sorry, there was an API error.\", []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return \"Sorry, an unexpected error occurred.\", []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"Enter your question (or 'exit' to quit): \")\n",
    "            if q.strip().lower() == \"exit\":\n",
    "                break\n",
    "            answer_text, hits = answer(q)\n",
    "            print(\"\\nAnswer:\\n\", answer_text)\n",
    "            print(\"\\nSources:\")\n",
    "            for i, h in enumerate(hits, 1):\n",
    "                print(f\"[{i}] {h['meta']['source']}#chunk{h['meta']['order']}\")\n",
    "            print(\"-\" * 40)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee69841-8c6f-4453-bd5c-8d7d3a4caeec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
